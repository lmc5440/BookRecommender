{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Import the required dependencies from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#import numpy\n",
    "import numpy as np\n",
    "\n",
    "#import from keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "\n",
    "#import gradio\n",
    "import gradio as gr\n",
    "\n",
    "#import VADER\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Books Reviews dataset loaded:\n",
      "           Id                               Title  Price         User_id  \\\n",
      "0  0671551345  Night World: Daughters Of Darkness    NaN   ADB0JID2XRFYR   \n",
      "1  0671551345  Night World: Daughters Of Darkness    NaN             NaN   \n",
      "2  0671551345  Night World: Daughters Of Darkness    NaN             NaN   \n",
      "3  0671551345  Night World: Daughters Of Darkness    NaN  A1V0SFB3AXM8JK   \n",
      "4  0671551345  Night World: Daughters Of Darkness    NaN             NaN   \n",
      "\n",
      "                                       profileName review/helpfulness  \\\n",
      "0  Harmony-Faith Charisma Izabela Jazmyn McDonague                1/3   \n",
      "1                                              NaN                1/3   \n",
      "2                                              NaN                1/3   \n",
      "3                        K. Davis \"The Rose Bride\"                0/2   \n",
      "4                                              NaN                0/0   \n",
      "\n",
      "   review/score  review/time  \\\n",
      "0           5.0   1076457600   \n",
      "1           5.0   1043971200   \n",
      "2           3.0    960422400   \n",
      "3           1.0   1177718400   \n",
      "4           5.0    889920000   \n",
      "\n",
      "                                      review/summary  \\\n",
      "0                                   BEST BOOK EVER!!   \n",
      "1              one of the best night world books!!!!   \n",
      "2                    three sisters to die for.......   \n",
      "3                     Disappointing to say the least   \n",
      "4  The most charming, captivating work from LJ Sm...   \n",
      "\n",
      "                                         review/text  \n",
      "0  This is 1 of da bst books dat i have EVER read...  \n",
      "1  first of all i thought that this was one of lj...  \n",
      "2  Once started I couldn't put it down, literally...  \n",
      "3  This book is probably, in my opinion, one of (...  \n",
      "4  The plot and characters are incredible. Everyo...  \n",
      "\n",
      "GoodReads Books dataset loaded:\n",
      "                     title                         titleComplete  \\\n",
      "0        Project Hail Mary                     Project Hail Mary   \n",
      "1  The Talented Mr. Ripley  The Talented Mr. Ripley (Ripley, #1)   \n",
      "2           More Than This                        More Than This   \n",
      "3       After Forever Ends                    After Forever Ends   \n",
      "4     A Bird Without Wings                  A Bird Without Wings   \n",
      "\n",
      "                                         description  \\\n",
      "0  Ryland Grace is the sole survivor on a despera...   \n",
      "1  Since his debut in 1955, Tom Ripley has evolve...   \n",
      "2  A boy drowns, desperate and alone in his final...   \n",
      "3  Orphaned by her mother and brushed off by her ...   \n",
      "4  After an impoverished and indigent childhood, ...   \n",
      "\n",
      "                                              genres        isbn  \\\n",
      "0  ['Science Fiction Fantasy', 'Audiobook', 'Fant...  0593135202   \n",
      "1  ['Novels', 'Noir', 'Classics', 'Italy', 'Suspe...  0393332144   \n",
      "2  ['Queer', 'Fantasy', 'Contemporary', 'LGBT', '...  1406350486   \n",
      "3  ['Chick Lit', 'Fantasy', 'Coming Of Age', 'Con...         NaN   \n",
      "4  ['Contemporary', 'Contemporary Romance', 'Roma...         NaN   \n",
      "\n",
      "               publisher                  author  \\\n",
      "0       Ballantine Books           ['Andy Weir']   \n",
      "1  W. W. Norton  Company  ['Patricia Highsmith']   \n",
      "2       Walker Books Ltd        ['Patrick Ness']   \n",
      "3       Gingersnap Press      ['Melodie Ramone']   \n",
      "4             Smashwords      ['Roberta Pearce']   \n",
      "\n",
      "                                          characters  \\\n",
      "0                          ['Ryland Grace', 'Rocky']   \n",
      "1  ['Freddie Miles', 'Tom Ripley', 'Dickie Greenl...   \n",
      "2                                   ['Seth Wearing']   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                              places  \\\n",
      "0                 ['Tau Ceti System', 'Outer Space']   \n",
      "1  ['Italy', 'New York City, New York', 'Italian ...   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                       ratingHistogram  ratingsCount  reviewsCount  numPages  \\\n",
      "0  [1917, 5775, 29742, 116572, 266669]      420675.0       53538.0     476.0   \n",
      "1    [1483, 3902, 17161, 34467, 24270]       81283.0        5146.0     288.0   \n",
      "2    [1441, 3672, 12295, 23873, 21208]       62489.0        8194.0     480.0   \n",
      "3             [81, 119, 205, 365, 750]        1520.0         241.0     564.0   \n",
      "4                   [7, 6, 26, 49, 91]         179.0          31.0       NaN   \n",
      "\n",
      "  language  \n",
      "0  English  \n",
      "1  English  \n",
      "2  English  \n",
      "3  English  \n",
      "4  English  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Amazon Books Reviews dataset\n",
    "amazon_reviews_path = 'Resources/book_reviews.csv'  # Adjust file path as needed\n",
    "amazon_df = pd.read_csv(amazon_reviews_path)\n",
    "print(\"Amazon Books Reviews dataset loaded:\")\n",
    "print(amazon_df.head())\n",
    "\n",
    "# Load the GoodReads Books dataset (with Description and Genre)\n",
    "goodreads_path = 'Resources/goodreads_dataset.csv'  # Adjust file path as needed\n",
    "goodreads_df = pd.read_csv(goodreads_path)\n",
    "print(\"\\nGoodReads Books dataset loaded:\")\n",
    "print(goodreads_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Reviews DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17158 entries, 0 to 17157\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Id                  17158 non-null  object \n",
      " 1   Title               17158 non-null  object \n",
      " 2   Price               10897 non-null  float64\n",
      " 3   User_id             14573 non-null  object \n",
      " 4   profileName         14572 non-null  object \n",
      " 5   review/helpfulness  17158 non-null  object \n",
      " 6   review/score        17158 non-null  float64\n",
      " 7   review/time         17158 non-null  int64  \n",
      " 8   review/summary      17155 non-null  object \n",
      " 9   review/text         17158 non-null  object \n",
      "dtypes: float64(2), int64(1), object(7)\n",
      "memory usage: 1.3+ MB\n",
      "\n",
      "GoodReads DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14712 entries, 0 to 14711\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            14712 non-null  object \n",
      " 1   titleComplete    14712 non-null  object \n",
      " 2   description      14712 non-null  object \n",
      " 3   genres           14712 non-null  object \n",
      " 4   isbn             11738 non-null  object \n",
      " 5   publisher        14042 non-null  object \n",
      " 6   author           14710 non-null  object \n",
      " 7   characters       7177 non-null   object \n",
      " 8   places           5971 non-null   object \n",
      " 9   ratingHistogram  14709 non-null  object \n",
      " 10  ratingsCount     14709 non-null  float64\n",
      " 11  reviewsCount     14692 non-null  float64\n",
      " 12  numPages         14467 non-null  float64\n",
      " 13  language         14358 non-null  object \n",
      "dtypes: float64(3), object(11)\n",
      "memory usage: 1.6+ MB\n",
      "\n",
      "Missing values in Amazon Reviews:\n",
      "Id                       0\n",
      "Title                    0\n",
      "Price                 6261\n",
      "User_id               2585\n",
      "profileName           2586\n",
      "review/helpfulness       0\n",
      "review/score             0\n",
      "review/time              0\n",
      "review/summary           3\n",
      "review/text              0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in GoodReads dataset:\n",
      "title                 0\n",
      "titleComplete         0\n",
      "description           0\n",
      "genres                0\n",
      "isbn               2974\n",
      "publisher           670\n",
      "author                2\n",
      "characters         7535\n",
      "places             8741\n",
      "ratingHistogram       3\n",
      "ratingsCount          3\n",
      "reviewsCount         20\n",
      "numPages            245\n",
      "language            354\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display basic info about each dataset\n",
    "print(\"Amazon Reviews DataFrame Info:\")\n",
    "amazon_df.info()\n",
    "print(\"\\nGoodReads DataFrame Info:\")\n",
    "goodreads_df.info()\n",
    "\n",
    "# Check for missing values in each dataset\n",
    "print(\"\\nMissing values in Amazon Reviews:\")\n",
    "print(amazon_df.isnull().sum())\n",
    "print(\"\\nMissing values in GoodReads dataset:\")\n",
    "print(goodreads_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review/score\n",
       "5.0    10556\n",
       "4.0     3265\n",
       "3.0     1458\n",
       "1.0      979\n",
       "2.0      900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Get the number of reviews by score:\n",
    "amazon_df['review/score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (17158, 1), indices imply (17158, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder()\n\u001b[1;32m      8\u001b[0m encoded_ratings \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(amazon_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview/score\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m----> 9\u001b[0m encoded_ratings_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview/score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([amazon_df, encoded_ratings_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    859\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m             arrays,\n\u001b[1;32m    861\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 867\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    876\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    877\u001b[0m         {},\n\u001b[1;32m    878\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    882\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (17158, 1), indices imply (17158, 5)"
     ]
    }
   ],
   "source": [
    "#Group scores using OneHotEncoder:\n",
    "\n",
    "\n",
    "# # encoded_ratings_df = pd.DataFrame(encoded_ratings.toarray(), columns=encoder.get_feature_names_out(['review/score']))  # Convert to DataFrame\n",
    "# # final_df = pd.concat([amazon_df, encoded_ratings_df], axis=1)  # Concatenate the original DataFrame with the encoded DataFrame\n",
    "\n",
    "# encoder = OneHotEncoder()\n",
    "# encoded_ratings = encoder.fit_transform(amazon_df[['review/score']])\n",
    "# encoded_ratings_df = pd.DataFrame(encoded_ratings, columns=encoder.get_feature_names_out(['review/score']))\n",
    "# final_df = pd.concat([amazon_df, encoded_ratings_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Amazon Reviews shape: (9296, 10)\n",
      "Cleaned GoodReads shape: (3831, 14)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate entries from both datasets\n",
    "amazon_df_clean = amazon_df.drop_duplicates()\n",
    "goodreads_df_clean = goodreads_df.drop_duplicates()\n",
    "\n",
    "# Drop rows with missing values\n",
    "amazon_df_clean = amazon_df_clean.dropna()\n",
    "goodreads_df_clean = goodreads_df_clean.dropna()\n",
    "\n",
    "# Verify cleaning steps\n",
    "print(\"Cleaned Amazon Reviews shape:\", amazon_df_clean.shape)\n",
    "print(\"Cleaned GoodReads shape:\", goodreads_df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Amazon Reviews: Index(['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness',\n",
      "       'review/score', 'review/time', 'review/summary', 'review/text'],\n",
      "      dtype='object')\n",
      "Columns in GoodReads dataset: Index(['title', 'titleComplete', 'description', 'genres', 'isbn', 'publisher',\n",
      "       'author', 'characters', 'places', 'ratingHistogram', 'ratingsCount',\n",
      "       'reviewsCount', 'numPages', 'language'],\n",
      "      dtype='object')\n",
      "Merged DataFrame preview:\n",
      "                       title  \\\n",
      "0          Project Hail Mary   \n",
      "1    The Talented Mr. Ripley   \n",
      "2   Tell the Wolves I'm Home   \n",
      "3      P.S. I Still Love You   \n",
      "4  The House on Mango Street   \n",
      "\n",
      "                                       titleComplete  \\\n",
      "0                                  Project Hail Mary   \n",
      "1               The Talented Mr. Ripley (Ripley, #1)   \n",
      "2                           Tell the Wolves I'm Home   \n",
      "3  P.S. I Still Love You (To All the Boys I've Lo...   \n",
      "4                          The House on Mango Street   \n",
      "\n",
      "                                         description  \\\n",
      "0  Ryland Grace is the sole survivor on a despera...   \n",
      "1  Since his debut in 1955, Tom Ripley has evolve...   \n",
      "2  In this striking literary debut, Carol Rifka B...   \n",
      "3  Lara Jean didn’t expect to really fall for Pet...   \n",
      "4  Acclaimed by critics, beloved by readers of al...   \n",
      "\n",
      "                                              genres        isbn  \\\n",
      "0  ['Science Fiction Fantasy', 'Audiobook', 'Fant...  0593135202   \n",
      "1  ['Novels', 'Noir', 'Classics', 'Italy', 'Suspe...  0393332144   \n",
      "2  ['Coming Of Age', 'Contemporary', 'Realistic F...  0679644199   \n",
      "3  ['Audiobook', 'Chick Lit', 'Contemporary', 'Yo...  144242673X   \n",
      "4  ['Coming Of Age', 'Contemporary', 'Poetry', 'R...  0679734775   \n",
      "\n",
      "                                  publisher                  author  \\\n",
      "0                          Ballantine Books           ['Andy Weir']   \n",
      "1                     W. W. Norton  Company  ['Patricia Highsmith']   \n",
      "2                              Random House   ['Carol Rifka Brunt']   \n",
      "3  Simon & Schuster Books for Young Readers           ['Jenny Han']   \n",
      "4                                   Vintage     ['Sandra Cisneros']   \n",
      "\n",
      "                                          characters  \\\n",
      "0                          ['Ryland Grace', 'Rocky']   \n",
      "1  ['Freddie Miles', 'Tom Ripley', 'Dickie Greenl...   \n",
      "2  ['Finn Weiss', 'Danni Elbus', 'Toby Aldshaw', ...   \n",
      "3  ['Lara Jean', 'Kitty', 'Margot', 'Peter Kavins...   \n",
      "4                             ['Nenny', 'Esperanza']   \n",
      "\n",
      "                                              places  \\\n",
      "0                 ['Tau Ceti System', 'Outer Space']   \n",
      "1  ['Italy', 'New York City, New York', 'Italian ...   \n",
      "2  ['New York City, New York', 'Westchester, New ...   \n",
      "3                      ['Charlottesville, Virginia']   \n",
      "4                              ['Chicago, Illinois']   \n",
      "\n",
      "                        ratingHistogram  ratingsCount  reviewsCount  numPages  \\\n",
      "0   [1917, 5775, 29742, 116572, 266669]      420675.0       53538.0     476.0   \n",
      "1     [1483, 3902, 17161, 34467, 24270]       81283.0        5146.0     288.0   \n",
      "2     [2434, 6435, 25132, 54594, 49453]      138048.0       15491.0     360.0   \n",
      "3  [3828, 14220, 69988, 132580, 122455]      343071.0       27039.0     337.0   \n",
      "4    [6128, 15183, 46748, 60133, 42220]      170412.0       14951.0     110.0   \n",
      "\n",
      "  language review/text  \n",
      "0  English         NaN  \n",
      "1  English         NaN  \n",
      "2  English         NaN  \n",
      "3  English         NaN  \n",
      "4  English         NaN  \n",
      "Merged DataFrame shape: (7577, 15)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the common columns to verify merge keys\n",
    "print(\"Columns in Amazon Reviews:\", amazon_df_clean.columns)\n",
    "print(\"Columns in GoodReads dataset:\", goodreads_df_clean.columns)\n",
    "\n",
    "# Merge on GoodReads' 'isbn' and Amazon's 'Id'\n",
    "# We take only the necessary columns from Amazon (Id and review/text)\n",
    "merged_df = pd.merge(goodreads_df_clean,\n",
    "                     amazon_df_clean[['Id', 'review/text']],\n",
    "                     how='left',\n",
    "                     left_on='isbn',\n",
    "                     right_on='Id')\n",
    "\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Id'])\n",
    "\n",
    "print(\"Merged DataFrame preview:\")\n",
    "print(merged_df.head())\n",
    "print(\"Merged DataFrame shape:\", merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned datasets and merged data exported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Export the cleaned individual datasets and merged dataset to CSV files\n",
    "amazon_df_clean.to_csv('Resources/amazon_reviews_clean.csv', index=False)\n",
    "goodreads_df_clean.to_csv('Resources/goodreads_clean.csv', index=False)\n",
    "merged_df.to_csv('Resources/merged_books_data.csv', index=False)\n",
    "\n",
    "print(\"Cleaned datasets and merged data exported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset loaded. Sample:\n",
      "                       title  \\\n",
      "0          Project Hail Mary   \n",
      "1    The Talented Mr. Ripley   \n",
      "2   Tell the Wolves I'm Home   \n",
      "3      P.S. I Still Love You   \n",
      "4  The House on Mango Street   \n",
      "\n",
      "                                       titleComplete  \\\n",
      "0                                  Project Hail Mary   \n",
      "1               The Talented Mr. Ripley (Ripley, #1)   \n",
      "2                           Tell the Wolves I'm Home   \n",
      "3  P.S. I Still Love You (To All the Boys I've Lo...   \n",
      "4                          The House on Mango Street   \n",
      "\n",
      "                                         description  \\\n",
      "0  Ryland Grace is the sole survivor on a despera...   \n",
      "1  Since his debut in 1955, Tom Ripley has evolve...   \n",
      "2  In this striking literary debut, Carol Rifka B...   \n",
      "3  Lara Jean didn’t expect to really fall for Pet...   \n",
      "4  Acclaimed by critics, beloved by readers of al...   \n",
      "\n",
      "                                              genres        isbn  \\\n",
      "0  ['Science Fiction Fantasy', 'Audiobook', 'Fant...  0593135202   \n",
      "1  ['Novels', 'Noir', 'Classics', 'Italy', 'Suspe...  0393332144   \n",
      "2  ['Coming Of Age', 'Contemporary', 'Realistic F...  0679644199   \n",
      "3  ['Audiobook', 'Chick Lit', 'Contemporary', 'Yo...  144242673X   \n",
      "4  ['Coming Of Age', 'Contemporary', 'Poetry', 'R...  0679734775   \n",
      "\n",
      "                                  publisher                  author  \\\n",
      "0                          Ballantine Books           ['Andy Weir']   \n",
      "1                     W. W. Norton  Company  ['Patricia Highsmith']   \n",
      "2                              Random House   ['Carol Rifka Brunt']   \n",
      "3  Simon & Schuster Books for Young Readers           ['Jenny Han']   \n",
      "4                                   Vintage     ['Sandra Cisneros']   \n",
      "\n",
      "                                          characters  \\\n",
      "0                          ['Ryland Grace', 'Rocky']   \n",
      "1  ['Freddie Miles', 'Tom Ripley', 'Dickie Greenl...   \n",
      "2  ['Finn Weiss', 'Danni Elbus', 'Toby Aldshaw', ...   \n",
      "3  ['Lara Jean', 'Kitty', 'Margot', 'Peter Kavins...   \n",
      "4                             ['Nenny', 'Esperanza']   \n",
      "\n",
      "                                              places  \\\n",
      "0                 ['Tau Ceti System', 'Outer Space']   \n",
      "1  ['Italy', 'New York City, New York', 'Italian ...   \n",
      "2  ['New York City, New York', 'Westchester, New ...   \n",
      "3                      ['Charlottesville, Virginia']   \n",
      "4                              ['Chicago, Illinois']   \n",
      "\n",
      "                        ratingHistogram  ratingsCount  reviewsCount  numPages  \\\n",
      "0   [1917, 5775, 29742, 116572, 266669]      420675.0       53538.0     476.0   \n",
      "1     [1483, 3902, 17161, 34467, 24270]       81283.0        5146.0     288.0   \n",
      "2     [2434, 6435, 25132, 54594, 49453]      138048.0       15491.0     360.0   \n",
      "3  [3828, 14220, 69988, 132580, 122455]      343071.0       27039.0     337.0   \n",
      "4    [6128, 15183, 46748, 60133, 42220]      170412.0       14951.0     110.0   \n",
      "\n",
      "  language review/text  \n",
      "0  English         NaN  \n",
      "1  English         NaN  \n",
      "2  English         NaN  \n",
      "3  English         NaN  \n",
      "4  English         NaN  \n",
      "\n",
      "Combined text features preview:\n",
      "                       title  \\\n",
      "0          Project Hail Mary   \n",
      "1    The Talented Mr. Ripley   \n",
      "2   Tell the Wolves I'm Home   \n",
      "3      P.S. I Still Love You   \n",
      "4  The House on Mango Street   \n",
      "\n",
      "                                       text_features  \n",
      "0  Ryland Grace is the sole survivor on a despera...  \n",
      "1  Since his debut in 1955, Tom Ripley has evolve...  \n",
      "2  In this striking literary debut, Carol Rifka B...  \n",
      "3  Lara Jean didn’t expect to really fall for Pet...  \n",
      "4  Acclaimed by critics, beloved by readers of al...  \n",
      "\n",
      "TF-IDF matrix shape: (7577, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load the merged dataset\n",
    "merged_df = pd.read_csv('Resources/merged_books_data.csv')\n",
    "print(\"Merged dataset loaded. Sample:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Create a new column 'text_features' combining description and review text\n",
    "merged_df['text_features'] = merged_df['description'].fillna('') + \" \" + merged_df['review/text'].fillna('')\n",
    "print(\"\\nCombined text features preview:\")\n",
    "print(merged_df[['title', 'text_features']].head())\n",
    "\n",
    "# Use TF-IDF to convert text data to vectors\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(merged_df['text_features'])\n",
    "\n",
    "print(\"\\nTF-IDF matrix shape:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/laurenchristiansen/nltk_data'\n    - '/opt/anaconda3/envs/dev/nltk_data'\n    - '/opt/anaconda3/envs/dev/share/nltk_data'\n    - '/opt/anaconda3/envs/dev/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#assign sentiment score to review and description text\u001b[39;00m\n\u001b[1;32m      2\u001b[0m text_features \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sentiment_scores \u001b[38;5;241m=\u001b[39m text_features\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: analyzer\u001b[38;5;241m.\u001b[39mpolarity_scores(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, score \u001b[38;5;129;01min\u001b[39;00m sentiment_scores\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/nltk/sentiment/vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexicon_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/nltk/data.py:836\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    839\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/nltk/data.py:962\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    959\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/laurenchristiansen/nltk_data'\n    - '/opt/anaconda3/envs/dev/nltk_data'\n    - '/opt/anaconda3/envs/dev/share/nltk_data'\n    - '/opt/anaconda3/envs/dev/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#assign sentiment score to review and description text\n",
    "text_features = merged_df['text_features']\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = text_features.apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "for index, score in sentiment_scores.items():\n",
    "    print(f\"User review's sentiment score for review {index} is: {score}\")\n",
    "\n",
    "print(f\"User review's sentiment score is : {sentiment_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Vectorize text\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mtext_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Vectorize user's review\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tf_user \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_features\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text_features'"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# Vectorize text\n",
    "vectorized_text = vectorizer.fit_transform(text_features[\"text_features\"])\n",
    "# Vectorize user's review\n",
    "tf_user = vectorizer.transform([\"text_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the cosine similarity\n",
    "# similarity = cosine_similarity(tf_user, tf_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the reviews based on ratings according to user review's sentiment\n",
    "# if sentiment >= 0:\n",
    "#     reviews_filtered = reviews[reviews['review/score'] >= 4]\n",
    "# else:\n",
    "#     reviews_filtered = reviews[reviews['review/score'] < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between TF-IDF feature vectors\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Create a mapping from book title to index\n",
    "indices = pd.Series(merged_df.index, index=merged_df['title']).drop_duplicates()\n",
    "\n",
    "def recommend_books(title, cosine_sim=cosine_sim, df=merged_df, indices=indices, top_n=5):\n",
    "    # Get the index of the book that matches the title\n",
    "    idx = indices.get(title)\n",
    "    if idx is None:\n",
    "        return \"Book not found in our dataset.\"\n",
    "    \n",
    "    # Get the pairwise similarity scores of all books with that book\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Skip the first one since it is the book itself\n",
    "    sim_scores = sim_scores[1: top_n+1]\n",
    "    # Get the indices of the most similar books\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the top n most similar books\n",
    "    return df[['title', 'genre']].iloc[book_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6061\n",
      "Test samples: 1516\n"
     ]
    }
   ],
   "source": [
    "# Assume 'genre' is the target and 'text_features' is the input\n",
    "texts = merged_df['text_features'].astype(str)\n",
    "labels = merged_df['genres']\n",
    "\n",
    "# Encode genres into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_seq_length = 200  # adjust as necessary\n",
    "X = pad_sequences(sequences, maxlen=max_seq_length)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Test samples:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           640000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3368)              218920    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 863,080\n",
      "Trainable params: 863,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "  1/171 [..............................] - ETA: 30s - loss: 8.1215 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 13:05:05.870116: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 1s 4ms/step - loss: 6.8353 - accuracy: 0.0812 - val_loss: 5.8468 - val_accuracy: 0.0643\n",
      "Epoch 2/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 5.6595 - accuracy: 0.0618 - val_loss: 5.8594 - val_accuracy: 0.0725\n",
      "Epoch 3/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 5.4115 - accuracy: 0.0644 - val_loss: 6.2097 - val_accuracy: 0.0643\n",
      "Epoch 4/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 5.2121 - accuracy: 0.1139 - val_loss: 6.4304 - val_accuracy: 0.1400\n",
      "Epoch 5/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 4.9909 - accuracy: 0.1755 - val_loss: 6.5061 - val_accuracy: 0.2191\n",
      "Epoch 6/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 4.7300 - accuracy: 0.2076 - val_loss: 6.5794 - val_accuracy: 0.2339\n",
      "Epoch 7/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 4.4727 - accuracy: 0.2682 - val_loss: 6.6863 - val_accuracy: 0.3114\n",
      "Epoch 8/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 4.2323 - accuracy: 0.3051 - val_loss: 6.8054 - val_accuracy: 0.3377\n",
      "Epoch 9/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 4.0236 - accuracy: 0.3326 - val_loss: 7.0236 - val_accuracy: 0.3773\n",
      "Epoch 10/10\n",
      "171/171 [==============================] - 1s 4ms/step - loss: 3.8448 - accuracy: 0.3528 - val_loss: 7.3577 - val_accuracy: 0.3707\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=max_seq_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 7.1948 - accuracy: 0.3588\n",
      "Test Accuracy: 0.3588390648365021\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix for 17k reviews shape: (9296, 5000)\n"
     ]
    }
   ],
   "source": [
    "reviews = amazon_df_clean['review/text'].astype(str)\n",
    "\n",
    "# Create a TF-IDF vectorizer; you can adjust max_features as needed.\n",
    "tfidf_vectorizer_reviews = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the review texts into a numerical matrix.\n",
    "tfidf_reviews_matrix = tfidf_vectorizer_reviews.fit_transform(reviews)\n",
    "\n",
    "print(\"TF-IDF matrix for 17k reviews shape:\", tfidf_reviews_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming merged_df is our merged dataframe with a 'title' column.\n",
    "book_titles = merged_df['title'].unique().tolist()\n",
    "\n",
    "def user_review_input(book_title, user_review):\n",
    "    # Verify the book exists in our dataset\n",
    "    if book_title not in book_titles:\n",
    "        return \"Book not found in our dataset.\"\n",
    "    return f\"Review for '{book_title}':\\n{user_review}\"\n",
    "\n",
    "# Create a Gradio interface with two inputs: a dropdown for the book title and a textbox for the review.\n",
    "interface = gr.Interface(\n",
    "    fn=user_review_input, \n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=book_titles, label=\"Select Book Title\"),\n",
    "        gr.Textbox(lines=5, placeholder=\"Enter your review here...\", label=\"Your Review\")\n",
    "    ], \n",
    "    outputs=\"text\", \n",
    "    title=\"Book Review Input\",\n",
    "    description=\"Select a book and enter your review.\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "#Title, Header, Subheader\n",
    "st.title(\"Book Recommender\")\n",
    "st.header(\"Tell us about a book you like, and we'll recommend something you might like just as much!\")\n",
    "st.subheader(\"Please follow the prompts below.\")\n",
    "\n",
    "#Set selection box\n",
    "selection = st.selectbox()\n",
    "st.write(\"You've selected: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_reviews(user_review, top_n=5):\n",
    "    # Convert the user's review into its numerical (TF-IDF) representation.\n",
    "    user_review_vector = tfidf_vectorizer_reviews.transform([user_review])\n",
    "    \n",
    "    # Compute similarity scores between the user's review and all 17k reviews.\n",
    "    similarities = cosine_similarity(user_review_vector, tfidf_reviews_matrix)\n",
    "    \n",
    "    # Find the indices of the top_n most similar reviews.\n",
    "    similar_indices = similarities[0].argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Retrieve the matching review texts from the Amazon reviews dataset.\n",
    "    similar_reviews = amazon_df_clean.iloc[similar_indices][['review/text']].reset_index(drop=True)\n",
    "    \n",
    "    # Convert the result to a string for display.\n",
    "    return similar_reviews.to_string(index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_similarity_interface = gr.Interface(\n",
    "#     fn=get_similar_reviews,\n",
    "#     inputs=gr.Textbox(lines=5, placeholder=\"Enter your review text...\", label=\"Your Review\"),\n",
    "#     outputs=\"text\",\n",
    "#     title=\"Similar Reviews Finder\",\n",
    "#     description=\"Enter a review and see the top similar reviews from our dataset.\"\n",
    "# )\n",
    "\n",
    "# review_similarity_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/laurenchristiansen/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer.\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_review_sentiment(review_text, rating):\n",
    "    # Use VADER to compute sentiment scores.\n",
    "    sentiment_scores = sia.polarity_scores(review_text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    # Use the numeric rating to decide the review's sentiment.\n",
    "    if rating > 4:\n",
    "        rating_sentiment = \"Positive\"\n",
    "    else:\n",
    "        rating_sentiment = \"Negative\"\n",
    "    \n",
    "    # Format the result for display.\n",
    "    result_str = (\n",
    "        f\"Review: {review_text}\\n\"\n",
    "        f\"Compound Score: {compound_score}\\n\"\n",
    "        f\"Rating Sentiment (by rule): {rating_sentiment}\\n\"\n",
    "        f\"Full VADER Scores: {sentiment_scores}\"\n",
    "    )\n",
    "    return result_str\n",
    "\n",
    "# Create a Gradio interface that takes a review and a rating as inputs.\n",
    "sentiment_interface = gr.Interface(\n",
    "    fn=analyze_review_sentiment,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=5, placeholder=\"Enter your review text...\", label=\"Your Review\"),\n",
    "        gr.Slider(minimum=1, maximum=5, step=0.1, label=\"Rating\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"VADER Sentiment Analysis\",\n",
    "    description=\"Enter a review and its rating to see a sentiment analysis.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #histogram encoder\n",
    "# data = {\n",
    "#     'ratingHistogram': ['0-1', '1-2', '2-3', '3-4', '4-5']\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Initialize the OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# # Fit and transform the data\n",
    "# encoded_data = encoder.fit_transform(goodreads_df[['ratingHistogram']])\n",
    "\n",
    "# # Create a DataFrame with the encoded data\n",
    "# encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['ratingHistogram']))\n",
    "\n",
    "# # Concatenate the original DataFrame with the encoded DataFrame\n",
    "# result_df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# # Display the result\n",
    "# print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example DataFrame with ratings\n",
    "# goodreads_df = {'Film': ['Film A', 'Film B', 'Film C'],\n",
    "#         'Rating': [4.5, 3.5, 4.0]}\n",
    "# ratings_df = pd.DataFrame(ratings)\n",
    "\n",
    "# # Function to classify sentiment based on rating\n",
    "# def classify_sentiment(rating):\n",
    "#     if rating > 4:\n",
    "#         return 'Positive'\n",
    "#     else:\n",
    "#         return 'Negative'\n",
    "\n",
    "# # Apply the function to the Rating column\n",
    "# goodreads_df['Sentiment'] = df['Rating'].apply(classify_sentiment)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# # Import the spaCy library\n",
    "# import spacy\n",
    "# # Load the small English language model for spaCy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        This is 1 of da bst books dat i have EVER read! @ my school, we are doing a play on this & im playin Mary-Lynette. i cant wait 2 get to the last chapters when they finally give in 2 each other! Gr...\n",
      "1        first of all i thought that this was one of lj smith's best books she has written adn also the funniest. i love all the characters but my fave one in the book is Ash. he's really a hottie and a ba...\n",
      "2        Once started I couldn't put it down, literally. I didn't stop til I'd read it through.Three sisters on the run from the Night Worlds patriachal society, they visit Oregon. Their brother finds out ...\n",
      "3        This book is probably, in my opinion, one of (if not THE) worst in the Night World Series. It is Ash's story this time, who's soulmate just happens to be a human. (which Ironically was shadowed up...\n",
      "4                                         The plot and characters are incredible. Everyone that likes the supernatural should read this book, and all the other Night World books. I think I'm in love with Ash!\n",
      "                                                                                                          ...                                                                                                   \n",
      "17153    A close friend recommended us to read the book and the contents of the book has indeed lived up to its title. It is not just a fancy title to catch attention but the solid principles on male and f...\n",
      "17154    I've seen this material work over and over again. Divorce papers put through the shredder and deeper intimacy created. Whether you are struggling in marriage or just looking to create more intimac...\n",
      "17155    I hardly ever take the time to write reviews, but I cannot keep quiet about this resource. If you read nothing else regarding Marriage and relationships, READ THIS BOOK!!! (Or listen to the CD ver...\n",
      "17156    It is a profound mystery why in this day and age so many people refuse to recognize that there are certain fundamental differences between men and women. And, as the French might say, \"Vive la dif...\n",
      "17157    I'm engaged and got this as a Christmas present. Not only is it sexist to both women and men, some of the ideas in here are just offensive. There was one line about domestic abuse and how if a hus...\n",
      "Name: review/text, Length: 17158, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m reviews \u001b[38;5;241m=\u001b[39m reviews_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview/text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(reviews)\n\u001b[0;32m----> 5\u001b[0m spacy_reviews \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m spacy_reviews]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/spacy/language.py:1040\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1025\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/spacy/language.py:1134\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n\u001b[0;32m-> 1134\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1041\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "# # Tokenize the first sentence using token.text\n",
    "# reviews = reviews_df[\"review/text\"]\n",
    "# print(reviews)\n",
    "\n",
    "# spacy_reviews = nlp(reviews)\n",
    "# [token.text for token in spacy_reviews]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
